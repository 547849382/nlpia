= Packets of Thought (Basic NLP)
:chapter: 1
:part: 1
:imagesdir: .

In this chapter you'll learn

* What Natural Language Processing is
* Some of the magical things NLP can do
* Why NLP is hard and only recently has become widespread
* When word order and grammar is important and when it can be ignored
* How a chatbot combines many of the tools of NLP
* How to use a regular expression to build the start of a tiny ChatBot

You are about to embark on an exciting adventure in Natural Language Processing (NLP). 
First we will show you what Natural Language Processing is and all the things you can do with it. 
This will get your wheels turning, helping you think of ways to use Natural Language Processing in your own life both at work and at home.

Then we will dig into the details of exactly how to process a small bit of English text using a programming language like Python.
This will help you build up your NLP toolbox incrementally. 
In this chapter you'll write your first program that can read and write English statements.
This will be the first of many Python snippets that you will use to learn all the tricks needed to assemble an English language dialog engine, a chatbot.

== Natural Languages vs Programming Languages

Natural languages are different from computer programming languages. 
Natural languages aren't intended to be translated into a finite set of mathematical operations, like programming languages are. 
Natural languages are what humans use to share information with each other. 
We don't use programming languages to tell each other about our day to give directions to the grocery store. 
A computer program written with a programming language tells a machine exactly what to do.
But there are no compilers or interpreters for natural languages like English and French. 

.Natural Language Processing
[IMPORTANT,definition]
====
Natural Language Processing (NLP) is an area of research in Computer Science and Artificial Intelligence (AI) concerned with processing natural languages like English or Mandarin. This processing generally involves translating natural language into data (numbers) that a computer can use to learn about the world. And this understanding of the world is sometimes used generate natural language text that reflects that understanding.
====

Nonetheless this chapter will show you how a machine can **process** natural language. 
You might even think of this as a natural language interpreter, just like the Python interpreter. 
When the computer program you develop processes natural language it will be able to act on those statements or even reply to them. 
But these actions and replies are not precisely defined. 
That leaves more discretion up to you, the developer of the natural language pipeline. 

.Natural Language Pipeline
[IMPORTANT,definition]
====
A Natural Language Processing system is often referred to as a "pipeline" because it usually involves several stages of processing where natural language flows into one end of the pipeline and the processed output flows out at the other end.
====

You will soon have the power to write software that does interesting, unpredictable things, like carry on a conversation.
This can make machines seem a bit more human. 
It may seem a bit like magic.
At first, all advanced technology seems like magic. 
But we will pull back the curtain for you and help you explore around backstage. 
You'll soon discover all the props and tools you need to do the magic tricks yourself. 

[QUOTE,Dave Magee,Georgia Tech 1995]
====
"Everything is easy, once you know the answer."
====

== The Magic

What's so magical about a machine that can read and write in a natural language? 
Machines have been processing languages since computers were invented.
However, these were "formal" languages, programming languages designed to be interpreted (or compiled) only one correct way.
Ada, COBOL, and Fortran were some of the earliest languages.
Today Wikipedia lists more than 700 programming languages.
In contrast, _Ethnologue_ footnote:[_Ethnologue_ is a web-based publication that maintains statistics about natural languages.] has identified ten times as many natural languages spoken by humans around the world. 
And Google's index of natural natural language documents is well over 100 million gigabytes.footnote:[https://www.google.com/search/howsearchworks/crawling-indexing/]
And that's just the index.
And it's incomplete.
The size of the actual natural language content currently out there in the world wide web must exceed 100 billion gigabytes.footnote:[You can estimate the amount of actual natural language text out there to be at least 1000 times the size of Google's index.]
But this massive amount of natural language text isn't the only thing makes it so important to build software that can process it.

The interesting thing is that it's hard. 
It's just not natural for machines to be able to process something natural. 
It's kind of like building a building that can do something useful with architectural diagrams. 
It seems magical when software can process languages not designed for machines to understand. 
For one thing, it's one of the things we thought was a uniquely human capability. 

The word "natural" in "natural language" is used in the same sense that it is used in the term "natural world".
Natural, evolved things in the world about us are very different from mechanical, artificial things designed and built by humans.
Being able to design and build software that can read and process language like what you're reading here -- language about building software that can process natural language... well that's very meta, very magical.

To make your job a little easier we're going to focus on only one natural language, English.
But you can use the techniques you learn in this book to build software that can process any language, even a language that you do not understand, or has yet to be deciphered by archaeologists and linguists.
And we're going to show you how write software to process and generate that language using only one programming language, Python.

Python was designed from the ground up to be as readable a language as possible.
It also exposes a lot of its own language processing "guts."
Both of these characteristics make it a natural choice for learning natural language processing. 
And it makes Python a great language for building maintainable production pipelines for NLP algorithms in an enterprise environment, with many contributors to a single codebase. 
And we'll even use Python in lieu of the "universal language" of mathematics and mathematical symbols, wherever possible.
After all, Python is an unambiguous way to express mathematical algorithms.footnote:[mathematical notation is ambiguous: https://en.wikipedia.org/wiki/Ambiguity#Mathematical_notation]
And Python is designed to be as readable as possible for programmers like you.

=== Machines That Converse

Natural languages can't be directly translated into a precise set of mathematical operations. 
But they do contain information and instructions that can be extracted. 
And those pieces of information and instruction can be stored, indexed, searched, or immediately acted upon. 
One of those actions could be to generate a sequence of words in response to a statement. 
This will be the function of the "dialog engine" or chatbot that we'll help you build. 

We will focus entirely on English text documents and messages, not spoken statements. 
We are bypassing the conversion of spoken statements into text, "speech recognition" or "speech to text" (STT). 
We are also ignoring "speech generation" or "text-to-speech", converting text back into some human-sounding voice utterance. 
But you can still use what you learn in this book to build a voice interface or virtual assistant like Siri or Alexa, because speech-to-text and text-to-speech libraries are freely available. 
Android and iOS mobile operating systems provide high quality speech recognition and generation APIs and there are Python packages to accomplish similar functionality on a laptop or server.

If you want to build a customized speech recognition or generation system, that is a whole book in itself. 
It requires a lot of high quality labeled data, voice recordings annotated with their phonetic spellings and natural language transcriptions aligned with the audio files. 
Some of the algorithms you learn here might help, but most of the algorithms are quite different. 
So we'll leave this as an "exercise for the reader."

=== The Math

Processing natural language to extract useful information can be difficult. 
It requires tedious statistics bookkeeping. 
But that's what machines are for. 
And like many other technical problems, it's a lot easier once you know the answer. 
Machines still cannot perform most practical NLP tasks like conversation and reading comprehension as accurately and reliably as humans. 
So you might be able to tweak the algorithms you learn in this book to do some NLP tasks a bit better. 

The techniques you will learn, however, are powerful enough to create machines that can surpass humans in both accuracy and speed for some surprisingly subtle tasks. 
For example, you might not have guessed that recognizing sarcasm in an isolated Twitter message can be done more accurately by a machine than by a human.footnote:[Gonzalo-Ibanez et al found that educated and trained human judges could not match the performance of their simple classification algorithm of 68% reported in their ACM paper]. https://github.com/MathieuCliche/Sarcasm_detector:[Python code] and http://www.thesarcasmdetector.com/:[webapp] by Matthew Cliche at Cornell achieves similar accuracy >70%] 
Don't worry, humans are still better at recognizing humor and sarcasm within an ongoing dialog, due to our ability to maintain information about the context of a statement. 
But machines are getting better and better at maintaining context. 
And this book will help you incorporate context (metadata) into your NLP pipeline if you want to try your hand at advancing the state of the art.

Once we are able to extract structured numerical data, vectors, from natural language, we can take advantage of all the tools of mathematics and machine learning. 
We will use the same linear algebra tricks as the projection of 3D objects onto a 2D computer screen, something that computers and drafters were doing long before natural language processing came into its own. It's these breakthrough ideas that opened up a world of "semantic" analysis, allowing computers to interpret and store the "meaning" of statements rather than just counts of words or characters. 
Semantic analysis, along with statistics, can help resolve the ambiguity of natural language, the fact that words or phrases often have multiple meanings or interpretations.

So "extracting information" isn't at all like building a programming language compiler (fortunately for you). 
The most promising techniques bypass the rigid rules of regular grammars (patterns) or formal languages. 
We can rely on statistical relationships between words instead of a deep system of logical rules.footnote:[Some grammar rules can be implemented in a Computer Science abstraction called a "finite state machine." Regular grammars can be implemented in regular expressions. There are two Python packages for running regular expression finite state machines, `re` which is built in, and `regex` which must be installed, but may soon replace `re`. Finite state machines are just trees of "if...then...else" statements for each token (character/word/n-gram) or action that a machine needs to react to or generate.] 
Imagine if you had to define English grammar and spelling rules in a nested tree of "if...then" statements. 
Could you ever write enough rules to deal with every possible way that words, letters, and punctuation can be combined to make a statement?
Would this even begin to capture the semantics, the meaning of English statements? 
Even if it were useful for some kinds of statements, imagine how limited and brittle this software would be. 
Unanticipated spelling or punctuation would break or befuddle your algorithm. 

Natural languages have an additional "decoding" challenge that is even harder to solve. 
Speakers and writers of natural languages assume that a human is the one doing the processing (listening or reading), not a machine. 
So when I say "good morning", I assume that you have some knowledge about all the things that make up a morning including not only that mornings come before "noon"s and "afternoon"s and "evening"s but after "midnight"s. 
And you need to know they can represent times of day as well as general experiences of a period of time. 
And the interpreter is assumed to know that "good morning" is a common greeting that actually doesn't contain much information at all about the morning. 
Rather it reflects the state of mind of the speaker and her readiness to speak with others. 

This "theory of mind" about the human processor of language turns out to be a powerful assumption. 
It allows us to say a lot with very few words if we assume that the "processor" has access to a lifetime of common sense knowledge about the world. 
This degree of "compression" is still out of reach for machines. 
There is no clear "theory of mind" that you can point to in an NLP pipeline. 
However, we will show you techniques in later chapters to help machines build "ontologies" or knowledge bases of common sense knowledge to help interpret statements that rely on this knowledge.

== Practical Applications

Natural Language Processing (NLP) is everywhere. It's so ubiquitous that some of the examples here may surprise you.

.Categorized NLP applications 
[width="100%"]
|===========================================================================================
|*Search*             |Web                      |Documents             |Autocomplete
|*Editing*            |Spelling                 |Grammar               |Style
|*Dialog*             |Chatbot                  |Assistant             |Scheduling
|*Writing*            |Index                    |Concordance           |Table of Contents
|*Email*              |Spam filter              |Classification        |Prioritization
|*Text Mining*        |Summarization            |Knowledge extraction  |Medical diagnoses
|*Law*                |Legal inference          |Precedent search      |Subpoena classification
|*News*               |Event detection          |Fact checking         |Compose headlines
|*Attribution*        |Detect plagiarism        |Literary forensics    |Style coaching
|*Sentiment Analysis* |Monitor community morale |Product review triage |Customer care
|*Predict Behavior*   |Finance                  |Election forecasting  |Marketing
|*Creative writing*   |Movie scripts            |Poetry                |Song lyrics
|===========================================================================================


A search engine can provide more meaningful results if it indexes web pages or document archives in a way that takes into account the meaning of natural language text. 
"Autocomplete" uses NLP to "complete your thought" and is common among search engines and mobile phone keyboards. 
Many word processors, browser plugins, and text editors have spelling correctors, grammar checkers, concordance composers, and most recently, style coaches.
Some dialog engines (chatbots) use natural language search to find a response to their conversation partner's message. 

NLP pipelines that generate (compose) text can be used not only to compose short replies in chatbots and virtual assistants, but also to assemble much longer passages of text. 
The Associated Press uses NLP "robot journalists" to write entire financial news articles and sporting event reports.footnote:["AP's 'robot journalists' are writing their own stories now", The Verge, Jan 29, 2015, http://www.theverge.com/2015/1/29/7939067/ap-journalism-automation-robots-financial-reporting]
There are bots that compose weather forecasts that sound a lot like what your hometown weather person might say, perhaps because human meteorologists use word processors with NLP features to draft a script just before the show. 

NLP spam filters in early e-mail programs helped e-mail overtake telephone and fax communication channels in the 90s. 
And the spam filters have retained their edge in the cat and mouse game between spam filters and spam generators for e-mail, but may be losing in other environments like social networks. 
It's estimated that nearly 20% of the tweets about the 2016 US presidential election were composed by chatbots.footnote:[New York Times, Oct 18, 2016, https://www.nytimes.com/2016/11/18/technology/automated-pro-trump-bots-overwhelmed-pro-clinton-messages-researchers-say.html and MIT Technology Review, Nov 2016, https://www.technologyreview.com/s/602817/how-the-bot-y-politic-influenced-this-election/]
These bots amplify the viewpoint of the owners and developers of those bots with the resources and motivation to influence popular opinion. 
And these "puppet masters" tend to be foreign governments or large corporations.   
NLP systems can generate more than just 142-character social network posts. 
NLP can be used to compose lengthly movie and product reviews on Amazon and elsewhere. 
Many reviews are the creation of autonomous NLP pipelines that have never set foot in a movie theater or purchased the product they are reviewing. 

There are chatbots on Slack, IRC, and even customer service websites. 
These are places where chatbots have to deal with ambiguous commands or questions. 
And chatbots paired with voice recognition and generation systems can even handle lengthy conversations with an indefinite goal or "objective function" like making a reservation at a local restaurant.footnote:[Google Blog May 2018 about their _Duplex_ system https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html]
NLP systems can answer the phone for companies that want something better than a phone tree but don't want to pay a human to help their customers. 

[NOTE]
With its *Duplex* demonstration at Google IO, engineers and managers overlooked concerns about the ethics of teaching chatbots to deceive humans. We all ignore this dilemma when we happily interact chatbots on Twitter and other anonymous social networks, where bots do not share their pedigree. With bots that can so convincingly deceive us, the *AI control problem*footnote:[https://en.wikipedia.org/wiki/AI_control_problem] looms and Yuval Harari's cautionary forecast of "Homo Deus"footnote:[WSJ Blog, March 10, 2017 https://blogs.wsj.com/cio/2017/03/10/homo-deus-author-yuval-noah-harari-says-authority-shifting-from-people-to-ai/] may come sooner than we think.

There are NLP systems that can act as e-mail "receptionists" for businesses or executive assistants for managers. 
These assistants schedule meetings and record summary details in an electronic Rolodex, or CRM (Customer Relationship Management system), interacting with others by e-mail on their boss's behalf. 
Companies are putting the very brand and "face" of their company in the hands of NLP systems, allowing bots to execute marketing and messaging campaigns. 
And some inexperienced daredevil NLP textbook authors are letting bots author several sentences in their book. 
More on that later.

== Language through a Computer's "Eyes"

When we type "Good Morn'n Rosa", a computer sees only "01000111 01101111 01101111 ...". How can we program our chatbot to respond to this binary stream intelligently? Could a nested tree of conditionals (`if`... `else`..." statements) check each one of those bits and act on them individually? 
This would be equivalent to writing a special kind of program called a "finite state machine" (FSM). 
A finite state machine that outputs a sequence of new symbols as it runs, like the Python `str.translate` function, is called a  "finite state transducer" (FST). 
Don't worry if these terms are new to you. 
Things will become clear as we show you examples throughout this book. In fact you've probably already built a FSM without even knowing it. Have you ever written a regular expression? 
That's the kind of FSM that we'll use in the next section to show you one possible approach to NLP, the pattern-based approach.

What if we decided to just search a memory bank (database) for the exact same string of bits, characters, or words, and use one of the responses that other humans and authors have used for that statement in the past? But imagine if there was just one typo or variation in the statement. Our bot would be sent "off the rails." And bits aren't very continuous or forgiving, they either match or don't. There's no obvious way to find similarity between two streams of bits that takes into account what they actually signify. The bits for "Good" will be just as similar to "Bad!" as they are to "Okay".

But let's see how this approach would work before we show you a better way. Let's build a small regular expression to recognize greetings like "Good morning Rosa" and respond appropriately, our first tiny chatbot!

=== The Language of Locks (Regular Expressions)

Surprisingly the humble combination lock is actually a simple language processing machine. 
Hopefully you'll never think of your combination bicycle lock the same way again, after finishing this chapter. 
A combination lock certainly can't read and understand the textbooks stored inside a school locker, but it can understand the language of lock combinations. 
It can understand when you try to "tell" it a "password", a combination. 
A padlock combination is any sequence of symbols that matches the "grammar" (pattern) of lock language. 
Even more importantly it can tell if a combination lock "statement" matches a particularly meaningful statement, the one for which the right "answer" in the language of locks is to release the catch holding the U-shaped hasp so you can get into your locker.

And this "language of locks" is a kind of language, a particularly simple one, that we can use in a chatbot. 
We can use it to recognize a "key phrase" or command to unlock a particular action or behavior. 
For example, we'd like our chatbot to recognize greetings like "Hello Rosa," and respond to them appropriately. 
This kind of language, like the language of locks, is a "formal language" because it has strict rules about how an acceptable statement must be composed and interpreted. 
Formal languages are a subset of natural languages, so many statements in natural language can be captured by a formal language grammar. 
That's the reason for this diversion into the mechanical, "click, whirr"footnote:[one of Cialdini's six psychology principles in his popular book _Influence_ http://changingminds.org/techniques/general/cialdini/click-whirr.htm] language of combination locks.

We're going to use a slightly more restrictive grammar than formal grammar, called a "regular grammar." 
That grammar is particularly easy to work with. This small restriction in our language unlocks a broad set of powerful, easy-of-use features. 
Regular grammars have predictable, provable behavior, and yet are flexible enough to power some of the most sophisticated dialog engines and chatbots on the market. 
Amazon Alexa and Google Now are mostly pattern-based engines that rely on regular grammars. 
Deep, complex regular grammar rules can often be expressed in a single line of code called a "regular expression." 
There are very successful chatbot frameworks in Python, like `Will`, that rely exclusively on this kind of language to produce some useful and interesting behavior. 
Amazon Echo, Google Home, and similarly complex and useful "assistants" use this kind of language to encode the logic for most of their interaction with users.

[NOTE]
====
Actually, regular expressions implemented in Python and in Posix (Unix) applications like `grep`, are not true regular grammars. They have language and logic features like "look-ahead" and "look-back" that make leaps of logic and recursion that aren't allowed in a regular grammar. As a result, regular expressions aren't provably halting, they can sometimes "crash" or run forever.
====

You may be saying to yourself, "I've heard of regular expressions. I use `grep`. But that's only for search!" 
And you're right. **R**egular **E**xpressions are indeed used mostly for search, for sequence matching. 
But it turns out that anything that can find matches within text, is also great for carrying out a dialog. 
Some chatbots, like `Will`, use "search" to find sequences of characters within a user statement that they know how to respond to. 
These recognized sequences then trigger a scripted response appropriate to that particular regular expression "match." 
And that match can also be used to extract a useful piece of information from a statement so that a chatbot can add that bit of knowledge to its knowledge base about the user or about the world that the user is describing.

A machine that processes this kind of language can be thought of as formal mathematical object called a "finite state machine" (FSM) or "deterministic finite automaton" (DFA). 
FSMs will come up again and again in this book. 
So you'll eventually get a good feel for what they're used for without digging into FSM theory and math. 
For now, just think of them as combination locks. 
For those who can't resist trying to understand a bit more about these computer science tools, here's a diagram from Wikpedia that shows where FSMs fit into the nested world of automata (bots) and the "size" of the formal languages that each kind of automata can handle.

.Kinds of Automata
image::../images/ch01/kinds-of-automata.png[link="../images/ch01/kinds-of-automata.png"]

////
Redundant:
A mechanical example of a computer with a language and a handshake is a combination lock, like a padlock or safe. A combination padlock or safe is a mechanical computer with a grammar and language. You have to enter a sequence of numbers into a combination lock's brain to trigger an action, like pulling the pins on a safe or releasing the catch holding back a metal U bar. You have to enter these numbers in a particular order according to specific rules, a "grammar". For example, on some locks you have to rotate to the right 3 times before the first number, then to the left past the second number, and so on. The lock's vocabulary is the set of all the numbers and letters on its dial. The locks language, in the formal mathematical sense, is the set of all possible combinations of letters that the lock might be able to respond to, all the different combinations we could set in the lock: 01-01-01, 01-01-02, 01-01-03, etc. In Computer Science this is called a Finite State Machine, a machine that has a language and a grammar and can detect a match or "not match" for each sequence of signals (represented by characters or symbols or digits) that might be entered into the FSM. Computer programming languages like Python contain regular expressions to allow us to easily program the combination that "unlock"s some action or response. So let's use Python regular expressions to detect greetings like "Good morning Rosa", "Hello Hal" or even some hacky trigger phrase like "OK, Giggle."
////

=== A Simple Chatbot

Let's build a quick and very dirty chatbot.
It won't be very capable.
And it will require a lot of thinking about the English language and "hard-coding" of all the ways people may try to say something.
But don't worry if you think you couldn't have come up with this Python code yourself.
You won't have to try to think of all the different ways people can say something, like we did in this example.
You won't even have to write regular expressions (regexes).
We'll show you how build chatbot of your own in later chapters without hard-coding anything.
A modern chatbot can learn from just reading (processing) a bunch of English text.
And we'll show you how to do that in later chapters.
This is just an example of a very explicit approach, controlled approach that was common before modern chatbot techniques were developed.
And a variation of the approach we're going to show you here is what is behind chatbots like Amazon Alexa (though not the more sophisticated assistants out there).

For now let's build a Finite State Machine (FSM), a regular expression, that can speak "lock language" (regular language).
We could program it to understand lock language statements, like '01-02-03'.
Even better, we'd like it to understand greetings, things like "open sesame" or "hello Rosa."
An important feature for a prosocial chatbot is to be able to respond to a greeting.
In High School, teachers often chastised me for being impolite when I'd ignore greetings like this while rushing to class.
We surely don't want that for our kind and benevolent chatbot.

In machine communication protocol we would just define a simple "handshake" with an `ACK` (acknowledgement) signal after each message passed back and forth between two machines.
But our machines are going to be interacting with humans who say things like "Good morning, Rosa".
We don't want it sending out of bunch of chirps, beeps, or `ACK` messages, like it's syncing up a modem or HTTP connection at the start of a conversation or web browsing session.
Instead lets use regular expressions to recognize several different human greetings at the start of a conversation "handshake."

>>> import re  # <1>
>>> r = "(hi|hello|hey)[ ]*([a-z]*)"  # <2>
>>> re.match(r, 'Hello Rosa', flags=re.IGNORECASE)  # <3>
<_sre.SRE_Match object; span=(0, 10), match='Hello Rosa'>
>>> re.match(r, "hi ho, hi ho, it's off to work ...", flags=re.IGNORECASE)
<_sre.SRE_Match object; span=(0, 5), match='hi ho'>
>>> re.match(r, "hey, what's up", flags=re.IGNORECASE)
<_sre.SRE_Match object; span=(0, 3), match='hey>

<1> There are two "official" regular expression packages in Python. We're using the `re` package here just because it is installed with all versions of Python. The `regex` package will come with later versions of Python and is much more powerful as you'll see in Chapter 2. 
<2> `|` means "OR", `*` means the preceding character can occur 0 or more times and still match. So our regex will match greetings that start with "hi" or "hello" or "hey" followed by any number of `<space>` characters and then any number of letters.
<3> It's common to ignore the case of text characters to keep the regular expressions simpler

In regular expressions we can specify a "character class" with square brackets. 
And you can us a dash (`-`) to indicate a range of characters without having to type them all out individually. 
So the regular expression `"[a-z]"` will match any single lowercase letter, "a" through "z". 
The star (`*`) after a character class means that the regular expression will match any number of consecutive characters if they are all within that character class. 

Let's make our regular expression a lot more detailed to try to match more greetings. 

[source,python]
>>> r = r"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|afternoon|even[gin']{0,3}))[\s,;:]{1,3}([a-z]{1,20})"
>>> re_greeting = re.compile(r, flags=re.IGNORECASE)  # <1>
>>> re_greeting.match('Hello Rosa')
<_sre.SRE_Match object; span=(0, 10), match='Hello Rosa'>
>>> re_greeting.match('Hello Rosa').groups()
('Hello', None, None, 'Rosa')
>>> re_greeting.match("Good morning Rosa")
<_sre.SRE_Match object; span=(0, 17), match="Good morning Rosa">
>>> re_greeting.match("Good Manning Rosa")  # <2>
>>> re_greeting.match('Good evening Rosa Parks').groups() <3>
('Good evening', 'Good ', 'evening', 'Rosa')
>>> re_greeting.match("Good Morn'n Rosa")
<_sre.SRE_Match object; span=(0, 16), match="Good Morn'n Rosa">
>>> re_greeting.match("yo Rosa")
<_sre.SRE_Match object; span=(0, 7), match='yo Rosa'>

<1> You can compile regular expressions so you don't have to specify the options (`flags`) each time you use it. 
<2> Notice that this regular expression cannot recognize (match) words with typos
<3> Our chatbot can separate different parts of the greeting into `groups` but it will be unaware or Rosa's famous last name, because we don't have a pattern to match any characters after the first name.

[TIP]
The "`r`" before the quote stands for "raw string" not "regular expression". A Python raw string lets us send backslashes directly to the regular expression compiler without have to double-backslash (`\\`) all the special regular expression characters like spaces (` `) and curly braces or handlebars (`{}`).

There's a lot of logic packed into that first line of code, the regular expression. 
It gets the the job done for a surprising range of greetings. 
But it missed that "Manning" typo. 
This is one of the reasons NLP is hard. 
In machine learning and medical diagnostic testing that's called a "False Negative" classification error. Unfortunately, it will also match some statements that humans would be very unlikely to ever say. 
That's called a "False Positive," which is also a bad thing. 
Having both false positive and false negative errors means that our regular expression is both too liberal and too strict. 
This means that our regular expression is both too liberal and too strict. 
These mistakes could make our bot sound a bit dull, mechanical. 
We'd have to do a lot more work to refine the phrases that it matches to be more human-like. 

And this tedious work would be highly unlikely to ever succeed at capturing all the slang and misspellings people use. 
Fortunately, composing regular expressions by hand isn't the only way to train a chatbot. 
Stay tuned for more on that later (the entire rest of the book).
So we only use them when we need precise control over a chatbot's behavior, such as when issuing commands to a voice assistant on your mobile phone. 

But let's go ahead and finish up our one-trick chatbot by adding an output generator. 
It needs to say something. 
We'll use Python's string formatter to create a "template" for our chatbot response. 

[source,python]
>>> my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot', 'chatterbot'])
>>> curt_names = set(['hal', 'you', 'u'])
>>> greeter_name = ''  # <1>
...
>>> match = re_greeting.match(input())
...
>>> if match:
...     at_name = match.groups()[-1]
...     if at_name in curt_names:
...         print("Good one.")
...     elif at_name.lower() in my_names:
...         print("Hi {}, How are you?".format(greeter_name)

<1> We don't yet know who is chatting to the bot, and we won't worry about it here.

So if you run this little script and "chat" to our bot with a phrase like "Hello Rosa", it will respond by asking about your day.
If you use a slightly rude name to address the chatbot, she will be less responsive, but not inflammatory, to try to encourage politeness.footnote:[The idea for this defusing response originated with Viktor Frankl's _Man's Search for Meaning_, his https://en.wikipedia.org/wiki/Logotherapy[Logotherapy] approach to psychology and the many popular novels where a child protagonist like Owen Meany has the wisdom to respond to an insult with a response like this]
If you name someone else who might be monitoring the conversation on a party line or forum, the bot will keep quiet and allow you and whomever you are addressing to chat.
Obviously there's no one else out there watching our `input()` line, but if this were a function within a larger chatbot, you want to deal will these sorts of things.

Because of the limitations of computational resources, early NLP researchers had to use their human brain's computational power to design and hand-tune complex logical rules to extract information from a natural language string. 
This is called a pattern-based approach to NLP. 
The patterns don't have to be merely character sequence patterns, like our regular expression. 
NLP also often involves patterns of word sequences, or parts of speech, or other "higher level" patterns. 
The core NLP building blocks like stemmers and tokenizers as well as sophisticated end-to-end NLP dialog engines (chatbots) like Liza were built this way, from regular expressions and pattern matching. 
The "art" of pattern-matching approaches to NLP is coming up with elegant patterns that capture just what you want, without too many lines of regular expression "code."

[TIP]
.Classical Computational Theory of Mind
====
This classical NLP pattern-matching approach is based on the "Computational Theory of Mind" (CTM). CTM assumes that human-like NLP can be accomplished with a finite set of logical rules that are processed in series.footnote:[Stanford Encyclopedia of Philosophy, Computational Theory of Mind, https://plato.stanford.edu/entries/computational-mind/] Advancements in neuroscience and NLP led to the development of a "connectionist" theory of mind around the turn of the century. This theory allows for parallel pipelines processing natural language simultaneously, as is done in artificial neural networks.footnote:[Stanford Encyclopedia of Philosophy, Connectionism, https://plato.stanford.edu/entries/connectionism/] footnote:[Christiansen and Chater, 1999, Southern Illinois University,  https://crl.ucsd.edu/~elman/Bulgaria/christiansen-chater-soa.pdf]
====

You'll learn more about pattern-based approaches to tokenizing and stemming in Chapter 2. Perhaps you've heard of the "Porter Stemmer" or the "Treebank Tokenizer" which we'll show in Chapter 2. 
But in later chapters we're going to take advantage of the exponentially greater computational resources to process much larger, as well as our larger datasets, to shortcut this laborious hand programming and refining.

If you're new to regular expressions and want to learn more, you can check out the Regular Expression Appendix or the online documentation for Python regular expressions. But you don't have to to understand them just yet. We'll continue to provide you with example regular expressions as we use them for the building blocks of our NLP pipeline. So don't worry if they look like gibberish. Human brains are pretty good and generalizing from a set of examples and I'm sure it will become clear by the end of this book. And it turns out machines can learn this way as well...

=== Another Way

Is there a statistical or machine learning approach that might work in place of the pattern-based approach? 
If we had enough data could we do something different? 
What if we had a giant database containing sessions of dialog between humans, statements and responses for thousands or even millions of conversations? 
One way to build a chatbot would be to search that database for the exact same string of characters that our chatbot user just "said" to our chatbot. Couldn't we then use one of the responses to that statement that other humans have said in the past?

But imagine how a single typo or variation in the statement would trip up our bot. Bit and character sequences are discrete. 
They either match or don't. 
There's no obvious way to find similarity between two streams of bits that takes into account what they actually signify or mean. 
The bits and character sequences for "Good" will be just as similar to "Bad!" as they are to "Okay".

When we use character sequence matches to measure distance between natural language phrases, we'll often get it wrong. 
Phrases with similar meaning, like "Good" and "Okay", can often have very different character sequences and large distances when we count up character-by-character matches to measure distance. 
And sequences with completely different meanings, like "Bad" and "Bar", might be too close to one other when we use metrics designed to measure distances between numerical sequences. 
Metrics like Jaccard, Levenshtein, and Euclidean vector distance can sometimes add enough "fuzziness" to prevent a chatbot from stumbling over minor spelling errors or typos. 
But these metrics fail to capture the essence of the relationship between two strings of characters when they are very dissimilar. 
And they also sometime bring small spelling difference close together that might not really be typos, like "Bad" and "Bar". 

Distance metrics designed for numerical sequences and vectors are useful for a few NLP applications, like spelling correctors and popper name recognition. 
So we'll use these distance metrics when they make sense. 
But for NLP applications where we are more interested in the meaning of the natural language than its spelling, there are better approaches. 
We'll use vector representations of natural language words and text and some distance metrics for those vectors for those NLP applications. 
We'll show you each approach, one by one, as we talk about these different applications and the kinds of vectors they are used with. 

////
Diversion. Belongs in a later chapter.
A computational linguist or deep learning expert familiar with sequence-to-sequence models might quibble with that and claim that it's possible to build character-sequence distance metrics using a technique called "embedding." Different letters and sounds have subtle connotations, subjective but statistically significant shared meaning. "O" looks and sounds a bit softer than "D" or "B", so "O" is slightly more likely to be associated with words that have positive or softer sentiment, and this will be reflected in the statistics of the underlying bits that represent an ASCII "O" or "o". A metric could be constructed that took this into account and have a vague correspondence with the intended meaning, or at least the vague sentiment, emotion, of two statements. In later chapters we'll show you some of these techniques for identifying these subtle statistical correlations between sequences of symbols, whether they be bits, characters, words, or even sentences and entire documents. But there's another way, a quicker way.
////

We're not going to stay in this confusing binary world of logic for long, but let's imagine we're Mavis Batey at Bletchley park and we've just been handed that binary, Morse Code message intercepted from communication between two German military officers. It could hold the key to winning WWII. Where would we start? Well the first layer of deciding would be to do something statistical with that stream of bits to see if we can find patterns. Like Mavis Batey at Bletchley Park during WWII, we can first use the Morse Code table (or ASCII table, in our case) to assign letters to each group of bits. Then, if the characters are gibberish to us, as they are to a computer or a cryptographer in WWII, we could just start counting them up, looking up the short sequences in a dictionary of all the words we've seen before and putting a mark next to the entry every time it occurs. We might also make a mark in some other log book to indicate which message the word occurred in, creating an encyclopedic index to all the documents we've read before. This collection of documents is called a "corpus" and the words or sequences we've listed in our index are called a "lexicon".

If we're lucky, and we're not at war, and the messages we're looking at aren't strongly encrypted, we'll see patterns in those German word counts that mirror counts of English words used to communicate similar kinds of messages. Unlike a cryptographer trying to decipher German Morse code intercepts, we know that the symbols have consistent meaning and aren't changed with every key click to try to confuse us. This counting of characters and words is tedious work, but just the sort of thing a computer can do without thinking. And surprisingly, it's nearly enough to make the machine appear to understand our language. It can even do math on these statistical vectors that coincides with our human understanding of those phrases and words. When we show you how to teach a machine our language using "Word2Vec" in later chapters, it may seem magical, but it's not. It's just math, computation.

But let's think for a moment about what information has been lost in our effort to count all the words in the messages we receive. We assign the words to bins and store them away as bit vectors like a coin or token sorter directing different kinds of tokens to one side or the other in a cascade of decisions that piles them in bins at the bottom. Our sorting machine must take into account hundreds of thousands if not millions of possible token "denominations", one for each possible word that a speaker or author might use. Each phrase or sentence or document that we feed into our token sorting machine will come out the bottom where we have a "vector" with a count of the tokens in each slot. Most of our counts are zero, even for very large documents with verbose vocabulary. But we haven't lost any words yet. What have we lost? Could you, as a human understand a document that we presented you in this way, as a count each possible word in your language, without any sequence or order associated with those words? I doubt it. But if it was a short sentence or tweet, you'd probably be able to rearrange them into their intended order and meaning most of the time.

////
This is likely a copyrighted image. -HL
////

.Canadian Coin Sorter
image::../images/ch01/canadian-coin-sorter.jpg[Coin Sorting Tray,width=200,link="../images/ch01/canadian-coin-sorter.jpg"]

Here's how our token sorter fits into an NLP pipeline right after a tokenizer (see Chapter 2). We've included a stopword filter as well as a "rare" word filter in our mechanical token sorter sketch. Strings flow in from the top and bag-of-word vectors are created from the height profile of the token "stacks" at the bottom.

.Token Sorting Tray
image::../images/ch01/sketch-token-sorter.png[Token Sorting Tray,width=500,link="../images/ch01/sketch-token-sorter.png"]


It turns out that machines can handle this "bag of words" quite well and glean most of the information content of even moderately long documents this way. Each document, after token sorting and counting, can be represented as a vector, a sequence of integers for each word or token in that document. You see a couple crude examples below, and then well show some more useful data structures for bag-of-word vectors in Chapter 2.

////
The paragraph below is redundant. We'll consolidate this redundancy once we get feedback on all the various ways we say talk about vector representations and vector space models.
////

This is our first "vector space model" of a language. Those bins and the numbers they contain for each word are represented as very long vectors containing a lot of zeros and a few ones or twos scattered around wherever the word for that bin actually occurred. All the different ways that words could be combined to create these vectors is called a "vector space." And relationships between vectors in this space are what make up our model, which is attempting to predict combinations of these words occurring within a collection of various sequences of words (typically sentences or documents). In Python, we can represent these sparse (mostly empty) vectors (lists of numbers) as dictionaries. And a Python `Counter` is a special kind of dictionary that bins objects (including strings) and counts them just like we want.

[source,python]
----
>>> from collections import Counter

>>> Counter("Guten Morgen Rosa".split())
Counter({'Guten': 1, 'Rosa': 1, 'morgen': 1})
>>> Counter("Good morning, Rosa!".split())
Counter({'Good': 1, 'Rosa!': 1, 'morning,': 1})
----

You can probably imagine some ways we might clean those tokens up. We'll do just in the next chapter. But you might also think to yourself that these sparse, high-dimensional vectors (many bins, one for each possible word) aren't very useful for language processing. But they are good enough for some industry-changing tools like spam filters, which we'll discuss in Chapter 3.

And we can imagine feeding into this machine, one at a time, all the documents, statements sentences and even single words that we could find. We'd count up the tokens in each slot at the bottom after each of these statements were processed, and we'd call that a vector representation of that statement. All the possible vectors that a machine might create this way is called a "vector space." And this model of documents and statements and words is called a "vector space model". It allows us to use linear algebra to manipulate these vectors and compute things like distances and statistics about natural language statements. This helps us solve a much wider range of problems with less human programming and less brittleness in the NLP pipeline. One statistical question  that is asked of bag-of-word vector sequences is "What is the combination of words most likely to follow a particular bag-of-words." Or, even better, if a user enters a sequence of words, "What is the closest bag-of-words in our database to bag-of-words vector provided by the user?" This is a search query. The input words are the words you might type into a search box, and the closest bag-of-words vector corresponds to the document or web page you were looking for. The ability to efficiently answer these two questions would be sufficient to build a machine learning chatbot that could get better and better as we gave it more and more data.

But wait a minute, perhaps these vectors aren't like any you've ever worked with before. 
They're extremely high-dimensional. 
It's possible to have millions of dimensions for an 3-gram vocabulary computed from a large corpus. 
We'll discuss the curse of dimensionality in Chapter 3 and some other properties that make high dimensional vectors difficult to work with. 

== A Brief Overflight of Hyperspace

In chapter 3 we'll show you how to "consolidate" words into a smaller number of vector dimensions to help mitigate the curse of dimensionality and maybe turn it to our advantage. 
When we "project" these vectors onto each other to determine the distance between pairs of vectors this will be a reasonable estimate of the similarity in their **meaning** rather than merely just their statistical word usage. 
This vector distance metric is called "cosine distance metric" which we'll talk about in Chapter 3 and then reveal its true power on reduced dimension topic vectors in Chapter 4. 
We can even project ("embed" is the more precise term) these vectors in a 2-D plane to have a "look" at them in plots and diagrams to see if our human brains can find patterns. 
We can then teach a computer to recognize and act on these patterns in ways that reflect the underlying meaning of the words that produced those vectors. 

Imagine all the possible tweets or messages or sentences that humans might write. 
Even though we do repeat ourselves a lot, that's still a lot of possibilities. 
And when those tokens are each treated as separate, distinct dimensions, there's no concept that "Good morning, Hobs" has some shared meaning with "Guten Morgen, Hannes." 
We need to create some reduced dimension vector space model of messages so we can label them with a set of continuous (float) values. 
We could rate messages and words for qualities like subject matter and sentiment. We could ask questions like:

* How likely is this message to be a question?
* How much is it about a person? 
* How much is it about me? 
* How angry or happy does it sound? 
* Is it something I need to respond to? 

Think of all the ratings we could give statements. 
We could put these ratings in order and "compute" them for each statement to compile a "vector" for each statement. 
The list of ratings or dimensions we could give a set of statements should be much smaller than the number of possible statements, and statements that mean the same thing should have similar values for all of our questions.

These "rating" vectors become something that a machine can be programmed to react to. We can simplify and generalize our vectors further by clumping (clustering) statements together, making them close on some dimensions and not on others.

////
one-hot encoding: For example, if the dimensions were to be "does it contain the word good" and "does it contain the word 'morning'", then the phrase "good morning" would be a 1 on both these vectors - which would mean it's not one-hot encoding anymore, right? Anyway, the term 'one-hot vector' will be reused in Chapter 2. It's nicely explained there (I like the piano player analogy), but explaining that a one-hot vector is a vector of all zeros except for a single one, whose location encodes states which case we're representing, might help a bit here. Also, mentioning the advantages of one-hot vectors (like all vectors having the same distance) might be interesting, since the next time we'll be talking about one-hot encoding, we'll be jumping into the middle of it really quickly.` - RB
Yes, but one-hot coding of individual words are vectors can be added or ORed together to create a vector of zeros and ones to represent a sentence. No longer a one-hot vector, but a sum or binary OR, of one-hot vectors. I definitely need to do a better job of distinguishing between word vectors and statement/document vectors. - HL
////

But how can a computer assign values to each of these vector dimensions? Well, if we simplified our vector dimension questions to things like "does it contain the word "good"? Does it contain the word "morning"? Etc. You can see that we might be able to come up with a million or so questions resulting in numerical value assignments that a computer could make to a phrase. This is the first practical vector space model, called a bit vector language model, or the sum of "one-hot encoded" vectors. You can see why computers are just now getting powerful enough to make sense of natural language. The millions of million dimensional vectors that humans might generate simply "Does not compute!" on a supercomputer of the 80s, but is no problem on a commodity laptop in the 21st century. Actually it was more than just raw hardware power and capacity that made NLP practical--incremental, constant-RAM, linear algebra algorithms were the final piece of the puzzle that allowed machines to crack the code of natural language.

////
Does this representation below have any advantages? Also, would an illustration help here? - RB
////

Actually there's an even simpler, but much larger representation that can be used in a chatbot. 
What if our vector dimensions completely described the exact sequence of characters. It would contain the answer to questions like, "Is the first letter an A?" "Is it a B?"" ... "Is the second letter an A?"" and so on. This vector has the advantage that it retains all of the information contained in the original text, including the order of the characters and words. Imagine a player piano that could only play a single note at a time, and it had 52 or more possible notes it could play. The "notes" for this natural language mechanical player piano are the 26 uppercase and lowercase letters plus any punctuation that the piano must know how to "play." The paper roll wouldn't have to be much wider than for a real player piano and the number of notes in some long piano songs doesn't exceed the number of characters in a small document. But this one-hot character sequence encoding representation is mainly useful for recording and then replaying an exact "piece" rather than composing something new or extracting the "essence" of a piece. 
We can't easily compare the piano paper roll for one "song" to that of another. 
And this representation is actually longer than the original ascii-encoded representation of the document. 
The number of possible document representations just exploded in order to retain information about each sequence of characters. 
We retained the order of characters and words but expanded the dimensionality of our NLP problem. 

These representations of documents don't cluster together well, in this character-based vector world.  The Russian mathematician Vladimir Levenshtein came up with a brilliant approach for quickly finding similarities between vectors (strings of characters) in this world. Levenshtein's algorithm made it possible to create some surprisingly fun and useful chatbots, with only this simplistic, mechanical view of language. But the real magic happened when we figured out how to compress/embed these higher dimensional spaces into a lower dimensional space of fuzzy meaning or topic vectors. We'll peek behind the magician's curtain in chapter 4 when we talk about "Latent Semantic Indexing" and "Latent Dirichlet Allocation", two techniques for creating much more dense and meaningful vector representations of statements and documents.


== Word Order and Grammar

The order of words matters. Those rules that govern word order in a sequence of words (like a sentence) are called the "grammar" of a language. That's something that our "bag-of-words" or word vector discarded in the examples above. Fortunately, in most short phrases and even many complete sentences, this word vector approximation works OK. If you just want to encode the general sense and sentiment of a short sentence, word order is not terribly important. Take a look at all of these orderings of our "Good morning Rosa" example.

[source,python]
----
>>> from itertools import permutations

>>> [" ".join(combo) for combo in permutations("Good morning Rosa!".split(), 3)]
['Good morning Rosa!',
 'Good Rosa! morning',
 'morning Good Rosa!',
 'morning Rosa! Good',
 'Rosa! Good morning',
 'Rosa! morning Good']
----

Now if you tried to interpret each of those strings in isolation (without looking at the others) I think you'd probably conclude that they all probably had similar intent or meaning. You might even notice the capitalization of the word Good and thus place the word at the front of the phrase in your mind. But you might also think that "Good Rosa" was some sort of proper noun, like the name of a restaurant or flower shop. Nonetheless, a smart chatbot or clever woman of the 1940's in Bletchley Park would likely respond to any of these six permutations with the same innocuous greeting, "Good morning my dear General."

Let's try that (in our heads) on a much longer, more complex phrase, a logical statement where the order of the words matters a lot:

[source,python]
----
>>> s = "Find textbooks with titles containing 'NLP', or 'natural' and 'language', or 'computational' and  'linguistics'."
>>> len(set(s.split()))
12
>>> import numpy as np
>>> np.arange(1, 12 + 1).prod()  # factorial(12) = arange(1, 13).prod()
479001600
----

The number of permutations exploded from `factorial(3) == 6` in our simple greeting to `factorial(12) ==  479001600` in our longer statement! And it's clear that the logic contained in the order of the words is very important to any machine that would like to reply with the correct response. Even though common greetings are not usually garbled by bag-of-words processing, more complex statements can lose most of their meaning when thrown into a bag. A bag of words is not the best way to begin processing a database query, like the natural language query in the example above. Whether a statement is written in a formal programming language like SQL, or in an informal natural language like English, word order and grammar are very important when a statement intends to convey precise, logical relationships between things. That's why computer languages depend on rigid grammar and syntax rule parsers. Fortunately, recent advances in natural language syntax tree parsers have made it possible to extract syntactical and logical relationships from natural language with remarkable accuracy (greater than 90%).footnote:[A comparison of the syntax parsing accuracy of SpaCy (93%), SyntaxNet (94%), Stanford's CoreNLP (90%), and others is available at spacy.io/docs/api/] In later chapters we'll show you how to use packages like `SyntaxNet` (Parsey McParseface) and `SpaCy` to identify these relationships.

////
REDUNDANT:
For logical processing of natural language (inference) and question-answering systems (such as the natural language database query in the string above) you will need to take into account the order of words. Knowing the language's syntax and grammar becomes critical to resolving ambiguities in a natural language question and finding the correct answer. Later chapters will use statistical tools such as SyntaxNet and NLTK part-of-speech taggers to extract actionable information or knowledge from the grammar and word order of a sentence.
////

And just as in the Bletchley Park example greeting, even if a statement doesn't rely on word order for logical interpretation, sometimes paying attention to that word order can reveal subtle hints of meaning that might facilitate deeper responses. These deeper layers of natural language processing are discussed in the next section. And Chapter 2 will show you a trick for incorporating some of the information conveyed by word-order into our word-vector representation. It will also show you how to refine the crude tokenizer used in the examples above (`str.split()`) to more accurately bin words into more appropriate slots within the word vector, so that strings like "good" and "Good" are assigned the same "bin", while separate bins can be allocated for tokens like "rosa" and "Rosa" but not "Rosa!".

////
IMPROVEMENT:
We haven't yet fulfilled the promise of case normalization that doesn't lower proper nouns. Need POS tagging for that to be possible, that's in some later chapter.
////

== A Chatbot Natural Language Pipeline

The NLP pipeline required to build a dialog engine, or chatbot, is very similar to the pipeline required to build a question answering system described in _Taming Text_.footnote:[Ingersol, Morton, and Farris, http://www.manning.com/books/taming-text/?a_aid=totalgood] However, some of the the algorithms listed within the five subsystem blocks may be new to you. We will help you implement these in Python to accomplish various NLP tasks essential for most applications, including chatbots.

////
@hobson: I have seen the step of Analyzing intend. Maybe that should be added to the figure under 2. 
The abbreviations don't refer back to introduced terms (e.g. CNN, GAN, RBM, RNN), maybe the terms should be more general.
I would remove GAN from 2.  
////

.Chatbot Recirulating (Recurrent) Pipeline
image::../images/ch01/chatbot-pipeline.png[Chatbot Recirculating (Recurrent) Pipeline,align="center",width=70%,link="../images/ch01/chatbot-pipeline.png"]

A chatbot requires four kinds of processing as well as a database to maintain a memory of past statements and responses. Each of the four processing stages can contain one or more processing algorithms working in parallel or in series.

1. Parse: Extract features, structured numerical data, from natural language text
2. Analyze: Generate and combine features by scoring text for sentiment, grammaticality, semantics
3. Generate: Compose possible responses using templates, search, or language models
4. Execute: Plan statements based on conversation history and objectives, and select the next response

Each of these four stages can be implemented using one or more of the algorithms listed within the corresponding boxes in the block diagram. We will show you how to use Python to accomplish near state-of-the-art performance for each of these processing steps. And we'll show you several alternative approaches to implementing these five subsystems.

Most chatbots will contain elements of all five of these subsystems (the four processing stages as well as the database). However many applications only require simple algorithms for many of these steps. Some chatbots are better at answering factual questions and others are better at generating lengthy, complex, convincingly human responses. Each of these capabilities require different approaches and we will show you techniques for both.

In addition, deep learning and data-driven programming (machine learning, or "probabilistic language modeling") have rapidly diversified the possible applications for NLP and chatbots. This data-driven approach allows ever greater sophistication for an NLP pipeline simply by providing it with greater and greater amounts of data in the domain you want to apply it to. And when a new machine learning approach is discovered that makes even better use of this data, with more efficient model generalization or regularization, then large jumps in capability are possible.

The NLP pipeline for a chatbot shown in the diagram above contains all the building blocks for most of the NLP applications that we described at the start of this chapter. As in _Taming Text_ we have broken out our pipeline into 4 main subsystems or stages. In addition we've explicitly called out a database to record data required for each of these stages and persist their configuration and training sets over time. This can enable batch or online retraining of each of the stages as the chatbot interacts with the world. In addition we've shown a "feedback loop" on our generated text responses so that our responses can be processed using the same algorithms used to process the user statements. The response "scores" or features can then be combined in an objective function to evaluate and select the best possible response, depending on the chatbot's plan or goals for the dialog. This book is focused on configuring this NLP pipeline for a chatbot, but you may also be able to see the analogy to the NLP problem of text retrieval or "search", perhaps the most common NLP application. And our chatbot pipeline is certainly appropriate for the question answering application which was the focus of _Taming Text_.

The application of this pipeline to financial forecasting or business analytics may not be so obvious. But if your analysis subsystem (stage 2 above) is trained to generate features, scores, that are designed to be useful for your particular finance or business predictions, they can help you incorporate natural language data into a machine learning pipeline for forecasting. So, despite focusing on building a chatbot, this book will give you the tools to build a pipeline useful for a broad range of NLP applications, from search to forecasting.

One processing element in the diagram above that is not typically employed in search, forecasting, or question answering systems is natural language *generation*. For chatbots this is their central feature. Nonetheless, the text generation step is often incorporated into a search engine NLP application and can give such an engine a large competitive advantage. The ability to consolidate or summarize search results is a winning feature for many popular search engines (DuckDuckGo, Bing, and Google). And you can imagine how valuable it is for a financial forecasting engine to be able to generate statements, tweets, or entire articles based on the business-actionable events it detects in natural language streams from social media networks and news feeds.

The next section will show how the layers of such a system can be combined to create greater sophistication and capability at each stage of the NLP pipeline.


== Processing in Depth

The stages of a Natural Language Processing pipeline can be thought of as layers, like the layers in a feed-forward neural network. Deep Learning is all about creating more complex models and behavior by adding additional processing layers to the conventional 2-layer machine learning model architecture of feature extraction followed by modeling. In Chapter 5 we'll explain how neural networks help spread the learning across layers by backpropagating model errors from the output layers back to the input layers. But here we'll talk about the top layers and the what can be done by training each layer independently of the other layers.

image:../images/ch01/nlp-layers.png[link="../images/ch01/nlp-layers.png"]

The top four layers in this diagram correspond to the first two stages in the chatbot pipeline, feature extraction and feature analysis, in the previous section.
For example the Part-of-Speech tagging (POS tagging), is one way to generate features within the "Analyze" stage of our chatbot pipeline.
POS tags are generated automatically by the default `SpaCY` pipeline which includes all of the top four layers in this diagram.
POS tagging is typically accomplished with a Finite State Transducer (FST) like the methods in the `nltk.tag` package.
If terms like FST or FSM (Finite State Machine) are confusing, just ignore them for now.
Eventually your brain will learn what they mean from the context where we mention them throughout this book.footnote:[Word2Vec, the focus of Chapter 6, learns the meaning of words based on their neighboring words in a sentence, only your brain is better at it, and require fewer examples to generalize, form a model of the word]

The bottom two layers (Entity Relationships and a Knowledge Base) are used to populate a database containing information (knowledge) about a particular domain. And the information extracted from a particular statement or document using all six of these layers can then be used in combination with that database to make inferences. Inferences are logical extrapolations from a set of conditions detected in the environment, like the logic contained in the statement of a chatbot user. This kind of "inference engine" in the deeper layers of this diagram are considered the domain of Artificial Intelligence, where machines can make inferences about their world and use those inferences to make logical decisions. However, chatbots can make reasonable decisions without this knowledge database, using only the algorithms of the upper few layers. And these decisions can combine to produce surprisingly human-like behaviors.

Over the next few chapters we'll dive down through the top few layers of NLP. The top 3 layers are all that is required to perform meaningful sentiment analysis, semantic search, and to build human-mimicking chatbots. In fact, it's possible to build a useful and interesting chatbot using only single layer of processing, using the text (character sequences) directly as the features for a language model. A chatbot that only does string matching and search is capable of participating in a reasonably convincing conversation, if given enough example statements and responses.

For example, the open source project `ChatterBot` simplifies this pipeline by merely computing the string "edit distance" (Levenshtein distance) between an input statement and the statements recorded in its database. If it's database of statement-response pairs contains a matching statement, the corresponding reply (from a previously "learned" human or machine dialog) can be reused as the reply to the latest user statement. For this pipeline, all that is required is step 3 of our chatbot pipeline, "Generate." And within this stage only a brute force search algorithm is required to find the best response. With this simple technique (no tokenization or feature generation required), `ChatterBot` can maintain a convincing conversion as the dialog engine for Salvius, a mechanical robot built from salvaged parts by Gunther Cox.footnote:[ChatterBot by Gunther Cox and others at https://github.com/gunthercox/ChatterBot]

`Will` is an open source Python chatbot framework by Steven Skoczen with a completely different approach.footnote:[Will a chatbot for HipChat by Steven Skoczen and the HipChat community https://github.com/skoczen/will]. `Will` can only be trained to respond to statements by programming it with regular expressions. This is the labor-intensive and data-light approach to NLP. This grammar-based approach is especially effective for question answering systems and task-execution assistant bots, like Lex, Siri, and Google Now. These kinds of systems overcome the "brittleness" of regular expressions by employing "fuzzy regular expressions"footnote:[The Python `regex` package is backward compatible with `re` and adds fuzziness among other features. It will replace the `re` in the future: https://pypi.python.org/pypi/regex. Similarly TRE `agrep` (approximate grep) is an alternative to the unix commandline application `grep`: https://github.com/laurikari/tre/] and other techniques for finding approximate grammar matches. Fuzzy regular expressions find the closest grammar matches among a list of possible grammar rules (regular expressions) instead of exact matches by ignoring some maximum number of insertion, deletion, and substitution errors. However, expanding the breadth and complexity of behaviors for a grammar-based chatbot requires a lot of human development work. Even the most advanced grammar-based chatbots, built and maintained by some of the largest corporations on the planet (Google, Amazon, Apple, Microsoft) remain in the middle of the pack for depth and breadth of chatbot IQ.

A lot of powerful things can be done with shallow NLP. And very little, if any, human supervision (labeling or curating of text) is required. Often a machine can be left to learn perpetually from its environment (the stream of words it can pull from Twitter or some other source).footnote:[Restricted Boltzmann Machines are often the model of choice in recent research into this sort of unsupervised feature extraction or "embedding" of character sequences. Neural nets are more commonly used for token or word sequence embeddings and language models. We'll visit these and other unsupervised models in a chapter on natural language embedding.]


== Natural Language IQ

Like human brainpower, the power of an NLP pipeline cannot be easily gaged with a single IQ score without considering multiple "smarts" dimensions. A common way to measure the capability of a robotic system is along the dimensions of "complexity of behavior" and "degree of human supervision required." But for a natural language processing pipeline the goal is to build systems that fully automate the processing of natural language, eliminating all human supervision (once the model is trained and "deployed"). So a better pair of IQ dimensions should capture the breadth and depth of the complexity of the natural language pipeline.

A consumer product chatbot or virtual assistant like Alexa or Allo is usually designed to have extremely broad knowledge and capabilities. However, the logic used to respond to requests tends to be very shallow, often consisting of a set of trigger phrases that all produce the same response with a single "if-then" decision branch. Alexa (and the underlying Lex engine) essentially behave like a single layer, flat tree of (if, elif, elif, ...) statements. On the other hand, the Google Translate pipeline (or any similar machine translation system) relies on a very deep tree of feature extractors, decision trees, and deep knowledge graphs connecting bits of knowledge about the world. Sometimes these feature extractors, decision trees, and knowledge graphs are explicitly programmed into the system, as in the NLP Layers diagram. Another approach rapidly overtaking this "hand-coded" pipeline is the Deep Learning data-driven approach, where these layers tend to be an emergent, self-organizing property of large neural networks.

You will use both approaches as we incrementally build an NLP pipeline for a focused, but deep chatbot dialog engine.
This will give you the skills you need to accomplish natural language processing in your "vertical" or application area.
Along the way you'll probably get ideas about how to expand the breadth of things this NLP pipeline can do.
The diagram below puts our chatbot in its place among the natural language processing systems that are already out there.
Imagine the chatbots you have interacted with.
Where do you think they might fit on a plot like this?
Have you attempted to gage their intelligence by probing them with difficult questions or something like an IQ test?
You'll get a chance to do exactly that in later chapters, to help you decide how your chatbot stacks up against some of the others in this diagram.

image:../images/ch01/nlp-iq.png[link="../images/ch01/nlp-iq.png"]

As you progress through this book you will be building the elements of a chatbot.
Chatbots require all of the tools of NLP to work well:

* Feature extraction to produce a vector space model
* Information extraction to be able to answer factual questions
* Semantic search to help our chatbot learn from previous human dialog
* Natural language generation to compose new, meaningful statements

Machine Learning gives us a way to trick machines into behaving as if we'd spent a lifetime programming them with hundreds of complex regular expressions.
We can teach a machine to respond to patterns similar to the patterns defined in regular expressions by merely providing it examples of statements and the responses we want to see from the chatbot.
And the "models" of language, the FSMs, produced by machine learning, are much better.
They are less picky about mispelings and typoz.

And machine learning NLP pipelines are easier to "program."
We don't have to anticipate every possible use of symbols in our language.
We just have to feed the training pipeline with examples of the phrases that match, and example phrases that don't match.
As long we label them during training, so that the chatbot knows which is which, it will learn to discriminate between them.
And there are even machine learning approaches that require little if any "labeled" data.

The rest of this book is about using machine learning to save us from having to anticipate all the ways people can say things in natural language.
And each chapter incrementally improves on the basic NLP pipeline for a chatbot that we introduced in this chapter.
In the next chapter we'll detect these same greetings and many many more using machine learning.

== Summary

* Good NLP may help save the world
* The meaning and intent of words can be deciphered by machines
* A smart NLP pipeline will be able to deal with ambiguity
* We can teach machines common sense knowledge without spending a lifetime training them
* Chatbots can be thought of as semantic search engines
* Regular expressions are useful for more than just search

In this chapter we've given you some exciting reasons to learn about natural language processing.
You want to help save the world, don't you?
And we've attempted to pique your interest with some practical NLP applications that are revolutionizing the way we communicate, learn, do business, and even think.
Building an effective chatbot requires an understanding of the important and useful NLP tools and techniques.
And the chatbot depth and breadth of capabilities can be built up incrementally.
It won't be long before you're able to build a systems that approaches human-like conversational behavior, as long as you stay within it's "domain" of knowledge.
And you should be able to see in upcoming chapters how to train a chatbot with any domain knowledge that interests you -- from finance and sports to psychology and literature.
If you can find a _corpus_ of writing about it, then you can train a machine to understand it.
As you are learning the tools of Natural Language Processing you'll be building an NLP pipeline that can help you accomplish your goals in business and in life.
